[config]
seed = 0

[eval]
problem = netease.gym.Problem	wuji.problem.mdp.wrap.random.reset_seed()

[gym]
make = PongNoFrameskip-v4
unwrapped = 0
wrapper = netease.gym.wrapper.atari.NoopResetEnv(env, 30)	netease.gym.wrapper.atari.MaxAndSkipEnv(env, 4)	netease.gym.wrapper.atari.wrap_deepmind(env)	netease.gym.wrapper.Cast(env)	netease.gym.wrapper.Normalize(env, std=255)	netease.gym.wrapper.image.StackZero(env, 4)
length = 100

[gym_state]
ram = 1
image = 1

[gym_weight_reward]
env = 1

[gym_weight_final_reward]
env = 1

[model]
root = ~/model/wuji
module = wuji.model.pth.conv.PongDebug
critic = wuji.model.pth.wrap.critic.conv.hidden0
init = wuji.model.pth.wrap.init.kaiming_normal

[train]
optimizer = torch.optim.Adam(params, lr, betas=(0.9, 0.999), eps=1e-8)
lr = 0
lr_ = 0
batch_size = 0
batch_size_ = 0
clip_grad_norm = 0.5

[rl]
discount = 0.99
discount_ = 0
wrap =

[opponent_train]
mode = eval

[pg]
prob_min = 0
prob_min_ = 0
prob_min_opponent_ = 0
norm = reward
agent_train = 
agent_eval = 
explore = 

[ac]
truncation_ = 0
gae = 0.95
loss_critic = mse_loss
truncation = 5
wrap =

[ac_weight_loss]
policy = 1
policy_ = 0
critic = 0.5
critic_ = 0
entropy = 0.01
entropy_ = 0
